#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FSRA-VMKè¯„ä¼°è„šæœ¬
è¯„ä¼°è®­ç»ƒå¥½çš„Vision Mamba Kolmogorov Networkæ¨¡å‹åœ¨University-1652æ•°æ®é›†ä¸Šçš„æ€§èƒ½

æ”¯æŒçš„è¯„ä¼°æ¨¡å¼ï¼š
1. æ ‡å‡†è¯„ä¼°ï¼šè®¡ç®—Recall@Kã€mAPã€CMCç­‰æŒ‡æ ‡
2. å¯è§†åŒ–è¯„ä¼°ï¼šç”Ÿæˆæ£€ç´¢ç»“æœå¯è§†åŒ–
3. æ€§èƒ½åˆ†æï¼šåˆ†ææ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨
"""

import os
import sys
import time
import argparse
import yaml
import numpy as np
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from models.vmamba_kan_fsra import FSRAVMambaKAN
from dataset.university1652_dataset import University1652Dataset
from utils.metrics import (
    compute_distance_matrix, compute_recall_at_k, 
    compute_map, compute_cmc, plot_cmc_curve
)

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='FSRA-VMK Evaluation')
    
    # åŸºç¡€å‚æ•°
    parser.add_argument('--config', type=str, default='configs/fsra_vmk_config.yaml',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--checkpoint', type=str, required=True,
                        help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--data_dir', type=str, default='./data/University-1652',
                        help='æ•°æ®é›†æ ¹ç›®å½•')
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument('--query_mode', type=str, default='drone_to_satellite',
                        choices=['drone_to_satellite', 'satellite_to_drone'],
                        help='æŸ¥è¯¢æ¨¡å¼')
    parser.add_argument('--batch_size', type=int, default=64,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--image_size', type=int, default=256,
                        help='å›¾åƒå°ºå¯¸')
    
    # è¯„ä¼°é€‰é¡¹
    parser.add_argument('--flip_test', action='store_true',
                        help='ä½¿ç”¨ç¿»è½¬æµ‹è¯•å¢å¼º')
    parser.add_argument('--multi_scale_test', action='store_true',
                        help='ä½¿ç”¨å¤šå°ºåº¦æµ‹è¯•')
    parser.add_argument('--scales', type=float, nargs='+', default=[1.0],
                        help='æµ‹è¯•å°ºåº¦')
    parser.add_argument('--reranking', action='store_true',
                        help='ä½¿ç”¨é‡æ’åº')
    
    # å¯è§†åŒ–å’Œåˆ†æ
    parser.add_argument('--visualize', action='store_true',
                        help='ç”Ÿæˆæ£€ç´¢ç»“æœå¯è§†åŒ–')
    parser.add_argument('--num_vis_queries', type=int, default=10,
                        help='å¯è§†åŒ–çš„æŸ¥è¯¢æ•°é‡')
    parser.add_argument('--analyze_performance', action='store_true',
                        help='åˆ†ææ¨ç†æ€§èƒ½')
    parser.add_argument('--save_features', action='store_true',
                        help='ä¿å­˜æå–çš„ç‰¹å¾')
    
    # è¾“å‡ºè®¾ç½®
    parser.add_argument('--output_dir', type=str, default='./eval_results',
                        help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--device', type=str, default='cuda',
                        help='è¯„ä¼°è®¾å¤‡')
    
    return parser.parse_args()

class FSRAEvaluator:
    """FSRA-VMKè¯„ä¼°å™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(args.output_dir, exist_ok=True)
        
        # åŠ è½½é…ç½®
        if os.path.exists(args.config):
            with open(args.config, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
        else:
            self.config = {}
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self._init_dataloader()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_model()
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        self._load_checkpoint()
    
    def _init_dataloader(self):
        """åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨"""
        print("ğŸ”„ åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨...")
        
        # æŸ¥è¯¢é›†
        query_dataset = University1652Dataset(
            data_dir=self.args.data_dir,
            mode='query',
            query_mode=self.args.query_mode,
            image_size=self.args.image_size,
            augment=False
        )
        
        self.query_loader = DataLoader(
            query_dataset,
            batch_size=self.args.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        # ç”»å»Šé›†
        gallery_dataset = University1652Dataset(
            data_dir=self.args.data_dir,
            mode='gallery',
            query_mode=self.args.query_mode,
            image_size=self.args.image_size,
            augment=False
        )
        
        self.gallery_loader = DataLoader(
            gallery_dataset,
            batch_size=self.args.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        print(f"âœ… æŸ¥è¯¢é›†: {len(query_dataset)} æ ·æœ¬")
        print(f"âœ… ç”»å»Šé›†: {len(gallery_dataset)} æ ·æœ¬")
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯ç”¨äºå¯è§†åŒ–
        self.query_dataset = query_dataset
        self.gallery_dataset = gallery_dataset
    
    def _init_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        print("ğŸ”„ åˆå§‹åŒ–FSRA-VMKæ¨¡å‹...")
        
        # ä»é…ç½®æˆ–é»˜è®¤å€¼è·å–æ¨¡å‹å‚æ•°
        model_config = self.config.get('model', {})
        
        self.model = FSRAVMambaKAN(
            num_classes=self.config.get('dataset', {}).get('num_classes', 1652),
            embed_dim=model_config.get('embed_dim', 512),
            depth=model_config.get('depth', 12),
            kan_grid_size=model_config.get('kan', {}).get('grid_size', 5),
            num_heads=model_config.get('num_heads', 8),
            image_size=self.args.image_size
        ).to(self.device)
        
        print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def _load_checkpoint(self):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
        print(f"ğŸ”„ åŠ è½½æ£€æŸ¥ç‚¹: {self.args.checkpoint}")
        
        checkpoint = torch.load(self.args.checkpoint, map_location=self.device)
        
        # å…¼å®¹ä¸åŒçš„ä¿å­˜æ ¼å¼
        if 'model_state_dict' in checkpoint:
            self.model.load_state_dict(checkpoint['model_state_dict'])
            if 'best_map' in checkpoint:
                print(f"ğŸ“Š æ£€æŸ¥ç‚¹æœ€ä½³mAP: {checkpoint['best_map']:.4f}")
        else:
            self.model.load_state_dict(checkpoint)
        
        self.model.eval()
        print("âœ… æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆ")
    
    @torch.no_grad()
    def extract_features(self, dataloader, desc: str = "") -> Tuple[torch.Tensor, torch.Tensor, List[str]]:
        """æå–ç‰¹å¾"""
        print(f"ğŸ”„ æå–{desc}ç‰¹å¾...")
        
        all_features = []
        all_labels = []
        all_paths = []
        
        total_time = 0.0
        num_samples = 0
        
        for batch_idx, (images, labels, paths) in enumerate(dataloader):
            start_time = time.time()
            
            images = images.to(self.device)
            batch_size = images.size(0)
            
            # å¤šå°ºåº¦æµ‹è¯•
            if self.args.multi_scale_test:
                features_list = []
                for scale in self.args.scales:
                    if scale != 1.0:
                        h, w = images.shape[2], images.shape[3]
                        new_h, new_w = int(h * scale), int(w * scale)
                        scaled_images = F.interpolate(images, size=(new_h, new_w), 
                                                    mode='bilinear', align_corners=False)
                    else:
                        scaled_images = images
                    
                    outputs = self.model(scaled_images)
                    features_list.append(outputs['features'])
                
                # ç‰¹å¾èåˆ
                features = torch.stack(features_list).mean(dim=0)
            else:
                outputs = self.model(images)
                features = outputs['features']
            
            # ç¿»è½¬æµ‹è¯•
            if self.args.flip_test:
                flipped_images = torch.flip(images, dims=[3])  # æ°´å¹³ç¿»è½¬
                flipped_outputs = self.model(flipped_images)
                flipped_features = flipped_outputs['features']
                features = (features + flipped_features) / 2
            
            # L2å½’ä¸€åŒ–
            features = F.normalize(features, p=2, dim=1)
            
            all_features.append(features.cpu())
            all_labels.append(labels)
            all_paths.extend(paths)
            
            batch_time = time.time() - start_time
            total_time += batch_time
            num_samples += batch_size
            
            if batch_idx % 20 == 0:
                print(f"  å¤„ç†è¿›åº¦: [{batch_idx}/{len(dataloader)}] "
                      f"æ‰¹æ¬¡æ—¶é—´: {batch_time:.3f}s "
                      f"å¹³å‡FPS: {batch_size/batch_time:.1f}")
        
        all_features = torch.cat(all_features, dim=0)
        all_labels = torch.cat(all_labels, dim=0)
        
        avg_time_per_sample = total_time / num_samples
        print(f"âœ… {desc}ç‰¹å¾æå–å®Œæˆ: {all_features.shape}")
        print(f"â±ï¸  å¹³å‡æ¨ç†æ—¶é—´: {avg_time_per_sample*1000:.2f}ms/æ ·æœ¬")
        
        return all_features, all_labels, all_paths
    
    def compute_metrics(self, query_features: torch.Tensor, 
                       gallery_features: torch.Tensor,
                       query_labels: torch.Tensor, 
                       gallery_labels: torch.Tensor) -> Dict[str, float]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        print("ğŸ”„ è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        
        metrics = {}
        
        # Recall@K
        for k in [1, 5, 10, 20]:
            recall_k = compute_recall_at_k(query_features, gallery_features, 
                                         query_labels, gallery_labels, k=k)
            metrics[f'Recall@{k}'] = recall_k
        
        # mAP
        map_score = compute_map(query_features, gallery_features, 
                               query_labels, gallery_labels)
        metrics['mAP'] = map_score
        
        # CMCæ›²çº¿
        cmc = compute_cmc(query_features, gallery_features, 
                         query_labels, gallery_labels, max_rank=50)
        
        for i, rank in enumerate([1, 5, 10, 20, 50]):
            if i < len(cmc):
                metrics[f'CMC@{rank}'] = cmc[i]
        
        # å¹³å‡æ’åº
        distance_matrix = compute_distance_matrix(query_features, gallery_features)
        
        mean_rank = 0.0
        median_rank = []
        
        for i in range(len(query_labels)):
            # æ‰¾åˆ°æ­£ç¡®åŒ¹é…çš„galleryç´¢å¼•
            positive_mask = gallery_labels == query_labels[i]
            if positive_mask.sum() == 0:
                continue
            
            # è®¡ç®—æ’åº
            distances = distance_matrix[i]
            sorted_indices = torch.argsort(distances)
            
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ­£ç¡®åŒ¹é…çš„æ’åºä½ç½®
            for rank, idx in enumerate(sorted_indices):
                if positive_mask[idx]:
                    mean_rank += (rank + 1)
                    median_rank.append(rank + 1)
                    break
        
        metrics['MeanRank'] = mean_rank / len(query_labels)
        metrics['MedianRank'] = float(np.median(median_rank))
        
        return metrics, cmc
    
    def rerank_features(self, query_features: torch.Tensor, 
                       gallery_features: torch.Tensor,
                       k1: int = 20, k2: int = 6, lambda_value: float = 0.3) -> torch.Tensor:
        """k-reciprocalé‡æ’åº"""
        print("ğŸ”„ æ‰§è¡Œk-reciprocalé‡æ’åº...")
        
        # è®¡ç®—åŸå§‹è·ç¦»çŸ©é˜µ
        original_dist = compute_distance_matrix(query_features, gallery_features)
        
        # TODO: å®ç°å®Œæ•´çš„k-reciprocalé‡æ’åºç®—æ³•
        # è¿™é‡Œå…ˆè¿”å›åŸå§‹è·ç¦»çŸ©é˜µ
        return original_dist
    
    def visualize_results(self, query_features: torch.Tensor, 
                         gallery_features: torch.Tensor,
                         query_labels: torch.Tensor, 
                         gallery_labels: torch.Tensor,
                         query_paths: List[str],
                         gallery_paths: List[str]):
        """å¯è§†åŒ–æ£€ç´¢ç»“æœ"""
        print("ğŸ”„ ç”Ÿæˆæ£€ç´¢ç»“æœå¯è§†åŒ–...")
        
        # è®¡ç®—è·ç¦»çŸ©é˜µ
        distance_matrix = compute_distance_matrix(query_features, gallery_features)
        
        # éšæœºé€‰æ‹©æŸ¥è¯¢æ ·æœ¬è¿›è¡Œå¯è§†åŒ–
        query_indices = np.random.choice(len(query_labels), 
                                       min(self.args.num_vis_queries, len(query_labels)), 
                                       replace=False)
        
        vis_dir = os.path.join(self.args.output_dir, 'visualizations')
        os.makedirs(vis_dir, exist_ok=True)
        
        for i, query_idx in enumerate(query_indices):
            # è·å–æŸ¥è¯¢ä¿¡æ¯
            query_label = query_labels[query_idx].item()
            query_path = query_paths[query_idx]
            
            # è·å–æ£€ç´¢ç»“æœï¼ˆtop-10ï¼‰
            distances = distance_matrix[query_idx]
            sorted_indices = torch.argsort(distances)[:10]
            
            # åˆ›å»ºå¯è§†åŒ–å›¾åƒ
            fig, axes = plt.subplots(2, 6, figsize=(18, 6))
            fig.suptitle(f'Query {i+1}: Class {query_label}', fontsize=16)
            
            # æ˜¾ç¤ºæŸ¥è¯¢å›¾åƒ
            try:
                from PIL import Image
                query_img = Image.open(query_path).convert('RGB')
                axes[0, 0].imshow(query_img)
                axes[0, 0].set_title('Query', fontweight='bold')
                axes[0, 0].axis('off')
            except:
                axes[0, 0].text(0.5, 0.5, 'Query Image\nNot Found', 
                              ha='center', va='center', transform=axes[0, 0].transAxes)
                axes[0, 0].axis('off')
            
            # éšè—ç¬¬ä¸€è¡Œå‰©ä½™çš„subplot
            for j in range(1, 6):
                axes[0, j].axis('off')
            
            # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
            for j, gallery_idx in enumerate(sorted_indices[:5]):
                gallery_label = gallery_labels[gallery_idx].item()
                gallery_path = gallery_paths[gallery_idx]
                distance = distances[gallery_idx].item()
                
                # åˆ¤æ–­æ˜¯å¦æ­£ç¡®åŒ¹é…
                is_correct = gallery_label == query_label
                color = 'green' if is_correct else 'red'
                
                try:
                    gallery_img = Image.open(gallery_path).convert('RGB')
                    axes[1, j].imshow(gallery_img)
                except:
                    axes[1, j].text(0.5, 0.5, 'Image\nNot Found', 
                                  ha='center', va='center', transform=axes[1, j].transAxes)
                
                axes[1, j].set_title(f'Rank {j+1}\nClass {gallery_label}\nDist: {distance:.3f}', 
                                   color=color, fontsize=10)
                axes[1, j].axis('off')
                
                # æ·»åŠ è¾¹æ¡†
                for spine in axes[1, j].spines.values():
                    spine.set_edgecolor(color)
                    spine.set_linewidth(3)
            
            # éšè—æœ€åä¸€ä¸ªsubplot
            axes[1, 5].axis('off')
            
            plt.tight_layout()
            plt.savefig(os.path.join(vis_dir, f'query_{i+1}_results.png'), 
                       dpi=150, bbox_inches='tight')
            plt.close()
        
        print(f"âœ… å¯è§†åŒ–ç»“æœä¿å­˜è‡³: {vis_dir}")
    
    def analyze_performance(self):
        """åˆ†ææ¨ç†æ€§èƒ½"""
        print("ğŸ”„ åˆ†ææ¨ç†æ€§èƒ½...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_input = torch.randn(1, 3, self.args.image_size, self.args.image_size).to(self.device)
        
        # é¢„çƒ­
        for _ in range(10):
            _ = self.model(test_input)
        
        # æµ‹é‡æ¨ç†æ—¶é—´
        torch.cuda.synchronize()
        times = []
        
        for _ in range(100):
            start_time = time.time()
            with torch.no_grad():
                _ = self.model(test_input)
            torch.cuda.synchronize()
            times.append(time.time() - start_time)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        mean_time = np.mean(times) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        std_time = np.std(times) * 1000
        min_time = np.min(times) * 1000
        max_time = np.max(times) * 1000
        
        # ç»Ÿè®¡æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        model_size = total_params * 4 / 1024 / 1024  # MB
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / 1024 / 1024  # MB
            memory_reserved = torch.cuda.memory_reserved() / 1024 / 1024  # MB
        else:
            memory_allocated = 0
            memory_reserved = 0
        
        performance_info = {
            'inference_time_ms': {
                'mean': mean_time,
                'std': std_time,
                'min': min_time,
                'max': max_time
            },
            'throughput_fps': 1000 / mean_time,
            'model_params': {
                'total': total_params,
                'trainable': trainable_params,
                'size_mb': model_size
            },
            'memory_usage_mb': {
                'allocated': memory_allocated,
                'reserved': memory_reserved
            }
        }
        
        print("ğŸ“Š æ€§èƒ½åˆ†æç»“æœ:")
        print(f"  æ¨ç†æ—¶é—´: {mean_time:.2f}Â±{std_time:.2f}ms")
        print(f"  ååé‡: {1000/mean_time:.1f} FPS")
        print(f"  æ¨¡å‹å‚æ•°: {total_params:,} ({model_size:.2f} MB)")
        print(f"  GPUå†…å­˜: {memory_allocated:.2f} MB")
        
        # ä¿å­˜æ€§èƒ½åˆ†æç»“æœ
        import json
        with open(os.path.join(self.args.output_dir, 'performance_analysis.json'), 'w') as f:
            json.dump(performance_info, f, indent=2)
        
        return performance_info
    
    def run_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("ğŸš€ å¼€å§‹FSRA-VMKæ¨¡å‹è¯„ä¼°...")
        print(f"ğŸ“Š è¯„ä¼°é…ç½®:")
        print(f"  - æŸ¥è¯¢æ¨¡å¼: {self.args.query_mode}")
        print(f"  - ç¿»è½¬æµ‹è¯•: {self.args.flip_test}")
        print(f"  - å¤šå°ºåº¦æµ‹è¯•: {self.args.multi_scale_test}")
        print(f"  - é‡æ’åº: {self.args.reranking}")
        print(f"  - è®¾å¤‡: {self.device}")
        print("=" * 80)
        
        start_time = time.time()
        
        # æå–ç‰¹å¾
        query_features, query_labels, query_paths = self.extract_features(
            self.query_loader, "æŸ¥è¯¢é›†")
        gallery_features, gallery_labels, gallery_paths = self.extract_features(
            self.gallery_loader, "ç”»å»Šé›†")
        
        # ä¿å­˜ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
        if self.args.save_features:
            features_path = os.path.join(self.args.output_dir, 'features.pth')
            torch.save({
                'query_features': query_features,
                'query_labels': query_labels,
                'query_paths': query_paths,
                'gallery_features': gallery_features,
                'gallery_labels': gallery_labels,
                'gallery_paths': gallery_paths
            }, features_path)
            print(f"ğŸ’¾ ç‰¹å¾å·²ä¿å­˜è‡³: {features_path}")
        
        # é‡æ’åºï¼ˆå¯é€‰ï¼‰
        if self.args.reranking:
            # TODO: å®ç°é‡æ’åº
            pass
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics, cmc = self.compute_metrics(query_features, gallery_features, 
                                          query_labels, gallery_labels)
        
        # æ‰“å°ç»“æœ
        print("\nğŸ† è¯„ä¼°ç»“æœ:")
        print("=" * 50)
        for metric, value in metrics.items():
            print(f"  {metric:12s}: {value:.4f}")
        print("=" * 50)
        
        # ç»˜åˆ¶CMCæ›²çº¿
        plt.figure(figsize=(10, 6))
        plot_cmc_curve(cmc, max_rank=20)
        plt.title(f'CMC Curve - {self.args.query_mode}')
        plt.savefig(os.path.join(self.args.output_dir, 'cmc_curve.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # å¯è§†åŒ–ç»“æœï¼ˆå¯é€‰ï¼‰
        if self.args.visualize:
            self.visualize_results(query_features, gallery_features,
                                 query_labels, gallery_labels,
                                 query_paths, gallery_paths)
        
        # æ€§èƒ½åˆ†æï¼ˆå¯é€‰ï¼‰
        if self.args.analyze_performance:
            performance_info = self.analyze_performance()
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        results = {
            'metrics': metrics,
            'config': vars(self.args),
            'query_mode': self.args.query_mode,
            'num_queries': len(query_labels),
            'num_gallery': len(gallery_labels)
        }
        
        import json
        with open(os.path.join(self.args.output_dir, 'evaluation_results.json'), 'w') as f:
            json.dump(results, f, indent=2)
        
        total_time = time.time() - start_time
        print(f"\nâ±ï¸  è¯„ä¼°å®Œæˆï¼Œæ€»ç”¨æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.args.output_dir}")

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    print("=" * 80)
    print("ğŸ” FSRA-VMKæ¨¡å‹è¯„ä¼°")
    print("ğŸ“‹ è¯„ä¼°é…ç½®:")
    for arg, value in sorted(vars(args).items()):
        print(f"  {arg}: {value}")
    print("=" * 80)
    
    # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œè¯„ä¼°
    evaluator = FSRAEvaluator(args)
    evaluator.run_evaluation()

if __name__ == '__main__':
    main() 